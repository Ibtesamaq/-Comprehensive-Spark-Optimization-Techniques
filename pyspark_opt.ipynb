{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "from pyspark.sql import SparkSession"
      ],
      "metadata": {
        "id": "DY2RSwTdqB4Z"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "spark = SparkSession.builder.appName(\"pyspark optimization techniques\").getOrCreate()"
      ],
      "metadata": {
        "id": "KVOL1VdDqKVB"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "spark.conf.set(\"spark.sql.optimizer.dynamicPartitionPruning.enabled\",\"true\")  #enabling dpp"
      ],
      "metadata": {
        "id": "Mvn-IRi4xSQC"
      },
      "execution_count": 28,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Dynamic partiton pruning"
      ],
      "metadata": {
        "id": "HxBpMOPx0mHY"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark.sql.functions import col\n",
        "sales = spark.range(0, 100000).withColumn(\"region\", (col(\"id\") % 10).cast(\"string\"))\n",
        "sales.write.partitionBy(\"region\").mode(\"overwrite\").parquet(\"/tmp/sales_data\")\n",
        "\n",
        "# Read partitioned data\n",
        "sales_df = spark.read.parquet(\"/tmp/sales_data\")\n",
        "\n",
        "# Create filter table\n",
        "targets = spark.createDataFrame([(\"1\",), (\"2\",)], [\"region\"])"
      ],
      "metadata": {
        "id": "1sx6bbMzyuq_"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "sales_df.alias(\"s\").join(targets.alias(\"t\"), \"region\").select(\"id\").show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "S-CPzLJT0jy4",
        "outputId": "d645f598-75c9-45c7-d359-58026f8f2763"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+-----+\n",
            "|   id|\n",
            "+-----+\n",
            "|49991|\n",
            "|49981|\n",
            "|49971|\n",
            "|49961|\n",
            "|49951|\n",
            "|49941|\n",
            "|49931|\n",
            "|49921|\n",
            "|49911|\n",
            "|49901|\n",
            "|49891|\n",
            "|49881|\n",
            "|49871|\n",
            "|49861|\n",
            "|49851|\n",
            "|49841|\n",
            "|49831|\n",
            "|49821|\n",
            "|49811|\n",
            "|49801|\n",
            "+-----+\n",
            "only showing top 20 rows\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Predicate pushdown"
      ],
      "metadata": {
        "id": "LyxGysUL0yt_"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "filter pushdown and predicate pushdown are same terms in pyspark, same effects"
      ],
      "metadata": {
        "id": "DTCQXwhpCXzh"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark.sql.functions import col\n",
        "import time\n",
        "\n",
        "df = spark.range(0,10_000_000).withColumn(\"amount\", (col(\"id\")*2))\n",
        "df.write.mode(\"overwrite\").parquet(\"/tmp.parquet_data\")\n",
        "\n",
        "start = time.time()\n",
        "\n",
        "df_p = spark.read.parquet(\"/tmp.parquet_data\")\n",
        "df_p.filter(col(\"amount\")>19_999_000).show()\n",
        "\n",
        "print(\"time with predicate pushown will be:\", round(time.time()-start,2),\"sec\")\n",
        "\n",
        "\n",
        "#project pushdown\n",
        "df = spark.read.parquet(\"/tmp.parquet_data\")\n",
        "df.select(\"amount\", \"id\").show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "733M6yau4qsQ",
        "outputId": "1eeb06e4-0383-4cab-b946-04901aa59b94"
      },
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+-------+--------+\n",
            "|     id|  amount|\n",
            "+-------+--------+\n",
            "|9999501|19999002|\n",
            "|9999502|19999004|\n",
            "|9999503|19999006|\n",
            "|9999504|19999008|\n",
            "|9999505|19999010|\n",
            "|9999506|19999012|\n",
            "|9999507|19999014|\n",
            "|9999508|19999016|\n",
            "|9999509|19999018|\n",
            "|9999510|19999020|\n",
            "|9999511|19999022|\n",
            "|9999512|19999024|\n",
            "|9999513|19999026|\n",
            "|9999514|19999028|\n",
            "|9999515|19999030|\n",
            "|9999516|19999032|\n",
            "|9999517|19999034|\n",
            "|9999518|19999036|\n",
            "|9999519|19999038|\n",
            "|9999520|19999040|\n",
            "+-------+--------+\n",
            "only showing top 20 rows\n",
            "\n",
            "time with predicate pushown will be: 0.76 sec\n",
            "+--------+-------+\n",
            "|  amount|     id|\n",
            "+--------+-------+\n",
            "|10000000|5000000|\n",
            "|10000002|5000001|\n",
            "|10000004|5000002|\n",
            "|10000006|5000003|\n",
            "|10000008|5000004|\n",
            "|10000010|5000005|\n",
            "|10000012|5000006|\n",
            "|10000014|5000007|\n",
            "|10000016|5000008|\n",
            "|10000018|5000009|\n",
            "|10000020|5000010|\n",
            "|10000022|5000011|\n",
            "|10000024|5000012|\n",
            "|10000026|5000013|\n",
            "|10000028|5000014|\n",
            "|10000030|5000015|\n",
            "|10000032|5000016|\n",
            "|10000034|5000017|\n",
            "|10000036|5000018|\n",
            "|10000038|5000019|\n",
            "+--------+-------+\n",
            "only showing top 20 rows\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#5M Customers 10k Products 10M transactions    Data generation time"
      ],
      "metadata": {
        "id": "3Rt9oe9YFj27"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark.sql.functions import col, expr, rand\n",
        "from pyspark.sql.types import IntegerType, FloatType, StringType\n",
        "import time\n",
        "\n",
        "# 5M Customers\n",
        "customers = spark.range(1, 5_000_001).withColumnRenamed(\"id\", \"customer_id\")\n",
        "\n",
        "# 10k Products with categories and prices\n",
        "products = spark.range(1, 10_001).withColumnRenamed(\"id\", \"product_id\") \\\n",
        "    .withColumn(\"category\", expr(\"concat('Category_', product_id % 20)\")) \\\n",
        "    .withColumn(\"price\", (rand() * 100 + 1).cast(\"float\"))\n",
        "\n",
        "# 10M Transactions\n",
        "transactions = spark.range(1, 10_000_001).withColumnRenamed(\"id\", \"txn_id\") \\\n",
        "    .withColumn(\"customer_id\", expr(\"cast(rand() * 5000000 + 1 as long)\")) \\\n",
        "    .withColumn(\"product_id\", expr(\"cast(rand() * 10000 + 1 as long)\")) \\\n",
        "    .withColumn(\"txn_amount\", (rand() * 200 + 1).cast(\"float\")) \\\n",
        "    .withColumn(\"txn_date\", expr(\"date_add('2020-01-01', cast(rand() * 1000 as int))\"))"
      ],
      "metadata": {
        "id": "J4FaJ3H9Cp-Z"
      },
      "execution_count": 21,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "start = time.time()\n",
        "\n",
        "customers.write.mode(\"overwrite\").parquet(\"/content/customers_parquet\")\n",
        "products.write.mode(\"overwrite\").parquet(\"/content/products_parquet\")\n",
        "transactions.write.mode(\"overwrite\").parquet(\"/content/transactions_parquet\")\n",
        "\n",
        "print(\"✅ Data generation complete in\", round(time.time() - start, 2), \"seconds\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Gyf52QX3Gpap",
        "outputId": "49008825-d7ad-4f52-d11a-6330bd33076f"
      },
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "✅ Data generation complete in 13.35 seconds\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "df = spark.read.parquet(\"/content/transactions_parquet\")\n",
        "df.show(15)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "sTtgOkcyG0Ri",
        "outputId": "236769ac-4685-4368-8089-509b095609ea"
      },
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+-------+-----------+----------+----------+----------+\n",
            "| txn_id|customer_id|product_id|txn_amount|  txn_date|\n",
            "+-------+-----------+----------+----------+----------+\n",
            "|5000001|    2360511|      7498| 159.32028|2021-06-30|\n",
            "|5000002|    2677145|      7864| 3.5967855|2020-01-01|\n",
            "|5000003|    1757886|      9583| 1.6146764|2021-03-31|\n",
            "|5000004|    3448918|      1379| 170.29713|2020-04-20|\n",
            "|5000005|     717033|      3445| 170.64154|2020-06-04|\n",
            "|5000006|    4810544|      4824| 56.250557|2022-08-09|\n",
            "|5000007|    4587612|      1799| 177.81328|2021-07-05|\n",
            "|5000008|    1912982|      7795| 100.88956|2022-04-26|\n",
            "|5000009|    1819159|       114| 153.16435|2022-04-01|\n",
            "|5000010|    3689421|      1364| 165.82265|2020-04-21|\n",
            "|5000011|    1498691|      6662|105.492714|2021-07-29|\n",
            "|5000012|    3087565|      4258|  186.7567|2021-06-15|\n",
            "|5000013|     729633|      8765|  51.58265|2020-10-26|\n",
            "|5000014|    4508528|      5995| 181.63846|2020-03-14|\n",
            "|5000015|    2696518|      3199| 93.597984|2022-09-15|\n",
            "+-------+-----------+----------+----------+----------+\n",
            "only showing top 15 rows\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "transactions = spark.read.parquet(\"/content/transactions_parquet\")\n",
        "\n",
        "sorted_txns = transactions.orderBy(\"txn_amount\")\n",
        "\n",
        "sorted_txns.show(10)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7DVuheUMH3Fg",
        "outputId": "afb580c6-1e0b-4c60-fe46-c3ce61eda57c"
      },
      "execution_count": 26,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+-------+-----------+----------+----------+----------+\n",
            "| txn_id|customer_id|product_id|txn_amount|  txn_date|\n",
            "+-------+-----------+----------+----------+----------+\n",
            "|1042120|    1097813|      5960| 1.0000049|2020-03-24|\n",
            "|9761303|    3985205|      4798| 1.0000216|2020-08-12|\n",
            "|5996180|    4921044|      9159|  1.000022|2021-04-01|\n",
            "|2226240|    3895459|      3447| 1.0000298|2020-02-24|\n",
            "|2780339|    1305272|      5288| 1.0000516|2020-12-18|\n",
            "|8139010|    1695054|      9159| 1.0000608|2021-08-14|\n",
            "|1091272|    4268923|      5756| 1.0000753|2022-05-29|\n",
            "|1269259|      81727|       715| 1.0000994|2022-06-20|\n",
            "|7894708|     387144|      9071| 1.0001372|2021-12-21|\n",
            "|3875477|    2349743|      2759| 1.0001428|2021-05-19|\n",
            "+-------+-----------+----------+----------+----------+\n",
            "only showing top 10 rows\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "WSCG Whole stage code generation"
      ],
      "metadata": {
        "id": "K3obqPu1QvDx"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "spark.conf.get(\"spark.sql.codegen.wholeStage\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 36
        },
        "id": "CsH0-yQe58lj",
        "outputId": "7564d140-5cdd-4b3b-9bbb-e4c368f343ec"
      },
      "execution_count": 32,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'true'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 32
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#wholestage code generation checking\n",
        "\n",
        "from pyspark.sql.functions import col\n",
        "\n",
        "df = spark.range(1, 1_000_000).withColumn(\"value\", col(\"id\") * 2)\n",
        "\n",
        "# Run some transformations\n",
        "filtered = df.filter(col(\"value\") > 1000).select(\"value\")\n",
        "\n",
        "\n",
        "filtered.explain()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "17KkVYWsPLAF",
        "outputId": "f0941574-f4a3-4002-b1d5-b9824d1d29c8"
      },
      "execution_count": 27,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "== Physical Plan ==\n",
            "*(1) Project [(id#457L * 2) AS value#459L]\n",
            "+- *(1) Filter ((id#457L * 2) > 1000)\n",
            "   +- *(1) Range (1, 1000000, step=1, splits=2)\n",
            "\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "how to enable AQE ADaptive query execution"
      ],
      "metadata": {
        "id": "j8k8U6OR0pfc"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "spark = SparkSession.builder \\\n",
        "    .appName(\"AQE_Off_Example\") \\\n",
        "    .config(\"spark.sql.adaptive.enabled\", \"false\") \\\n",
        "    .getOrCreate()\n"
      ],
      "metadata": {
        "id": "gqo0qMzV0pP0"
      },
      "execution_count": 30,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "how to enable kryo serilization"
      ],
      "metadata": {
        "id": "V-WcCZ5E0kcq"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "spark = SparkSession.builder \\\n",
        "    .appName(\"KryoExample\") \\\n",
        "    .config(\"spark.serializer\", \"org.apache.spark.serializer.KryoSerializer\") \\\n",
        "    .getOrCreate()"
      ],
      "metadata": {
        "id": "FFNG7A-ntd0U"
      },
      "execution_count": 29,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Avoiding UDFs (or using pandas UDFs where necessary)"
      ],
      "metadata": {
        "id": "ng-CNLrS0vos"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark.sql.functions import pandas_udf\n",
        "import pandas as pd"
      ],
      "metadata": {
        "id": "rdla2cxR4HCL"
      },
      "execution_count": 33,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "@pandas_udf(\"int\")\n",
        "def square_pandas(s: pd.Series) -> pd.Series:\n",
        "    return s * s\n",
        "\n",
        "df.withColumn(\"squared\", square_pandas(col(\"value\"))).show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "aKf88yua9LaR",
        "outputId": "39f13dd2-b85b-4cf9-c91e-d194afecc48f"
      },
      "execution_count": 35,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+---+-----+-------+\n",
            "| id|value|squared|\n",
            "+---+-----+-------+\n",
            "|  1|    2|      4|\n",
            "|  2|    4|     16|\n",
            "|  3|    6|     36|\n",
            "|  4|    8|     64|\n",
            "|  5|   10|    100|\n",
            "|  6|   12|    144|\n",
            "|  7|   14|    196|\n",
            "|  8|   16|    256|\n",
            "|  9|   18|    324|\n",
            "| 10|   20|    400|\n",
            "| 11|   22|    484|\n",
            "| 12|   24|    576|\n",
            "| 13|   26|    676|\n",
            "| 14|   28|    784|\n",
            "| 15|   30|    900|\n",
            "| 16|   32|   1024|\n",
            "| 17|   34|   1156|\n",
            "| 18|   36|   1296|\n",
            "| 19|   38|   1444|\n",
            "| 20|   40|   1600|\n",
            "+---+-----+-------+\n",
            "only showing top 20 rows\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Tuning spark.sql.shuffle.partitions"
      ],
      "metadata": {
        "id": "oG8Qj7ps_MPo"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#first method\n",
        "spark = SparkSession.builder \\\n",
        "    .appName(\"ShufflePartitionTuning\") \\\n",
        "    .config(\"spark.sql.shuffle.partitions\", \"100\") \\\n",
        "    .getOrCreate()"
      ],
      "metadata": {
        "id": "1WWMk0tV_L5R"
      },
      "execution_count": 36,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#second method will be to set it dynamically at run time\n",
        "spark.conf.set(\"spark.sql.shuffle.partitions\", 50)"
      ],
      "metadata": {
        "id": "vUOoONEED7ta"
      },
      "execution_count": 37,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Avoiding collect() on large datasets"
      ],
      "metadata": {
        "id": "Hu4GNVQYEJk6"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# BAD: Will collect all 10 million rows into driver's memory\n",
        "df_large = spark.range(1, 10_000_000)\n",
        "\n",
        "df_large.show(5)                # Just displays first 5 rows/best technique"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MzHvsp_WLU4p",
        "outputId": "212f5fc2-afa9-4cbe-9365-e7b6142c5a88"
      },
      "execution_count": 42,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+---+\n",
            "| id|\n",
            "+---+\n",
            "|  1|\n",
            "|  2|\n",
            "|  3|\n",
            "|  4|\n",
            "|  5|\n",
            "+---+\n",
            "only showing top 5 rows\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Reuse of DataFrames or intermediate results"
      ],
      "metadata": {
        "id": "Izc3eO05LqRy"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#cache\n",
        "df_filtered = df.filter(col(\"value\") > 1000).cache()"
      ],
      "metadata": {
        "id": "RfnaAUFhMMhp"
      },
      "execution_count": 44,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#persist\n",
        "df_filtered = df.filter(col(\"value\") > 1000).persist()"
      ],
      "metadata": {
        "id": "MveUQGZdMGux"
      },
      "execution_count": 46,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Writing Data in Optimal Partition Size (~128MB) (can be simulated)"
      ],
      "metadata": {
        "id": "SVPv3XX0Mwrc"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#we will use repartiton in order to dicide into small files instead of suing large file like ion GBs or eg 300Mb"
      ],
      "metadata": {
        "id": "LjB1Xw5SNXWx"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}